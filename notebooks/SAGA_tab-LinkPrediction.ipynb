{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdflib\n",
    "from rdflib import URIRef\n",
    "from rdflib.namespace import OWL, RDF, RDFS,XSD, Namespace\n",
    "import csv\n",
    "from torch_geometric.data import HeteroData\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From KG data to PyG HeteroData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of the notebook process the KG to obtain the HeteroData representation needed from PyG to fed data into the GNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"https://dbpedia.org/ontology/\"\n",
    "possible_types = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_type(relation):\n",
    "    r_split = relation.split(\"/\")\n",
    "    return r_split[len(r_split)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_property_type(property):\n",
    "    split_p = property.split(\"^^\")\n",
    "    p_type = str(split_p[1].split(\"#\")[1]).lower()\n",
    "    \n",
    "    if p_type.startswith(\"xsd:integer\"):\n",
    "        return(\"Integer\", split_p[0])\n",
    "    if p_type.startswith(\"xsd:string\"):\n",
    "        return(\"String\", split_p[0])\n",
    "    if p_type.startswith(\"xsd:double\"):\n",
    "        return(\"Double\", split_p[0])\n",
    "    if p_type.startswith(\"xsd:gYear\"):\n",
    "        return(\"Year\",split_p[0])\n",
    "    if p_type.startswith(\"xsd:date\"):\n",
    "        return(\"Date\",split_p[0])\n",
    "    return (\"\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ontology = rdflib.Graph()\n",
    "ontology.parse('dbpedia-ontologia.ttl', format='ttl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ontology.bind(\"dbo\", Namespace(\"http://dbpedia.org/ontology/\"))\n",
    "ontology.bind(\"dbr\", Namespace(\"http://dbpedia.org/resource/\"))\n",
    "ontology.bind(\"rdfs\", Namespace(\"http://www.w3.org/2000/01/rdf-schema#\"))\n",
    "ontology.bind(\"owl\", Namespace(\"http://www.w3.org/2002/07/owl#\"))\n",
    "ontology.bind(\"rdf\", Namespace(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_possible_types(subj_type, obj_type):\n",
    "    if (subj_type,obj_type) not in possible_types:\n",
    "        \n",
    "        q = \"SELECT DISTINCT ?property WHERE {\"+\\\n",
    "        \"{ ?property rdfs:domain dbo:\"+subj_type+\". ?property rdfs:range dbo:\"+obj_type+\\\n",
    "        \" .} UNION {dbo:\"+subj_type +\" rdfs:subClassOf ?superclass. dbo:\"+obj_type +\" rdfs:subClassOf  ?superclass2 .\"+\\\n",
    "        \"  ?property rdfs:domain ?superclass . ?property rdfs:range ?superclass2 \"+\\\n",
    "        \"} }\"\n",
    "        \n",
    "        result = ontology.query(q)\n",
    "        results = []\n",
    "        for res in result:\n",
    "            results.append(str(res[0]))\n",
    "        \n",
    "        q2 = \"SELECT DISTINCT ?property WHERE {\"+\\\n",
    "        \"{dbo:\"+subj_type +\" rdfs:subClassOf ?superclass. \"+\\\n",
    "        \" ?property rdfs:domain ?superclass . ?property rdfs:range dbo:\"+obj_type+\\\n",
    "        \" .} UNION {dbo:\"+obj_type +\" rdfs:subClassOf  ?superclass2 . ?property rdfs:domain dbo:\"+\\\n",
    "        subj_type+\" . ?property rdfs:range ?superclass2}}\"\n",
    "        \n",
    "        result = ontology.query(q2)\n",
    "\n",
    "        for res in result:\n",
    "            results.append(str(res[0]))\n",
    "\n",
    "        possible_types[(subj_type,obj_type)] = results\n",
    "        return results\n",
    "    return possible_types[(subj_type, obj_type)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_and_type = {}\n",
    "relations = []\n",
    "triples = []\n",
    "triple_properties=[]\n",
    "use_properties = False #to extract features for property nodes set this to True\n",
    "properties_and_types ={}\n",
    "\n",
    "# Process the Knowledge Graph\n",
    "g = rdflib.Graph()\n",
    "g.parse('dbpedia-complete.nt', format='nt')\n",
    "for s, p, o in g:\n",
    "    str_s = str(s)\n",
    "    str_p = str(p)\n",
    "    str_o = str(o)\n",
    "\n",
    "    if str_p != str(RDF.type):\n",
    "        if not str_s in entities_and_type.keys():\n",
    "            entities_and_type[(str_s)] =[]\n",
    "        if not str_p in relations:\n",
    "            relations.append(str_p)\n",
    "\n",
    "        if str_o.find('^^') == -1:\n",
    "            if not str_o in entities_and_type.keys():\n",
    "                entities_and_type[str_o]=[]\n",
    "            triples.append((str_s,str_p,str_o))\n",
    "        else:\n",
    "            if use_properties:\n",
    "                if str_s not in properties_and_types.keys():\n",
    "                    properties_and_types[str_s] =[]\n",
    "                p_type, p_value = get_property_type(str_o)\n",
    "                if (str_s,p_type, p_value) not in properties_and_types[str_s]:\n",
    "                    properties_and_types[str_s].append((get_type(str_p), p_type, p_value))\n",
    "                triples.append((str_s,str_p,str_o))\n",
    "        \n",
    "    else:\n",
    "        if str_s not in entities_and_type.keys():\n",
    "            entities_and_type[str_s] =[]\n",
    "        triples.append((str_s,str_p,str_o))\n",
    "        split_o = str_o.split('/')\n",
    "        #split_o = str_o.split('#')\n",
    "        entities_and_type[str_s].append(split_o[len(split_o)-1])\n",
    "\n",
    "for e in entities_and_type:\n",
    "    entities_and_type[e].sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disambiguate_multiple_types(entities_and_type, s,p,o): \n",
    "    for subtype_subj in entities_and_type[str(s)]:\n",
    "        if len(entities_and_type[str(o)]) > 1:\n",
    "            for subtype_obj in entities_and_type[str(o)]:\n",
    "                possible_rels = get_possible_types( subtype_subj, subtype_obj)\n",
    "\n",
    "                if len(possible_rels) == 0:\n",
    "                    continue   \n",
    "                \n",
    "                p = get_type(p)\n",
    "                for rel in possible_rels:\n",
    "                    if get_type(rel) == p:\n",
    "                        return (subtype_subj,subtype_obj)\n",
    "        else:\n",
    "            subtype_obj = entities_and_type[str(o)][0]\n",
    "            possible_rels = get_possible_types(subtype_subj, subtype_obj)\n",
    "            if len(possible_rels) == 0:\n",
    "                    continue\n",
    "            p = get_type(p)   \n",
    "            for rel in possible_rels:\n",
    "                if get_type(rel) == p:\n",
    "                    return (subtype_subj,  subtype_obj)\n",
    "        \n",
    "    return (\"\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_triples = []\n",
    "added_types = []\n",
    "\n",
    "for s,p,o in triples:\n",
    "    s1 = str(s)\n",
    "    p1 = str(p)\n",
    "    o1 = str(o)\n",
    "\n",
    "    if p != str(RDF.type):\n",
    "        #if s1 in list(properties_and_types.keys()):\n",
    "        if o1.find(\"^^\") != -1:\n",
    "            #x = properties_and_type[str(s)]\n",
    "            new_triples.append((s, p, o))\n",
    "        if (s1 in list(entities_and_type.keys()) and\n",
    "            o1 in list(entities_and_type.keys())):\n",
    "            #se è una relazione tra classi\n",
    "\n",
    "            #se il soggetto o l'oggetto ha più di un tipo \n",
    "            if len(entities_and_type[str(s)]) > 1 or len(entities_and_type[str(o)]) > 1 :\n",
    "                new_subj_type, new_obj_type = disambiguate_multiple_types(entities_and_type,s,p,o)\n",
    "                if((new_subj_type, new_obj_type) == (\"\",\"\") \n",
    "                    or new_subj_type == \"\" \n",
    "                    or new_obj_type ==\"\"):\n",
    "                    continue\n",
    "                \n",
    "                entities_and_type[str(s)] = [new_subj_type]\n",
    "                entities_and_type[str(o)] = [new_obj_type]\n",
    "\n",
    "                if s not in added_types:\n",
    "                    new_triples.append((s, str(RDF.type),prefix + new_subj_type))\n",
    "                    added_types.append(s)\n",
    "                if o not in added_types:\n",
    "                    new_triples.append((o,str(RDF.type),prefix + new_obj_type ))\n",
    "                    added_types.append(o)\n",
    "            else: \n",
    "                if s not in added_types:\n",
    "                    new_triples.append((s, str(RDF.type),prefix+entities_and_type[str(s)][0] ))\n",
    "                    added_types.append(s)\n",
    "                if o not in added_types and str(o).find(\"^^\") == -1:\n",
    "                    new_triples.append((o, str(RDF.type),prefix+entities_and_type[str(o)][0] ))\n",
    "                    added_types.append(o)\n",
    "            if(s,p,o) not in new_triples:\n",
    "                new_triples.append((s, p, o))\n",
    "    else:\n",
    "        if s not in added_types and len(entities_and_type[str(s)]) == 1: \n",
    "            #controllo solo s perché o è il tipo, verifico che non ci sia piu di\n",
    "            #un tipo altrimenti rimando l'aggiunta\n",
    "            new_triples.append((s, p, o))\n",
    "            added_types.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_properties_and_types = {}\n",
    "for s in list(properties_and_types.keys()):\n",
    "    for element in properties_and_types[s]:\n",
    "        s_class = entities_and_type[s]\n",
    "        if s not in new_properties_and_types:\n",
    "            new_properties_and_types[s] = []\n",
    "        new_properties_and_types[s].append((s_class[0], element[0], element[1], element[2]))\n",
    "\n",
    "properties_and_types = new_properties_and_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_types_count = {}\n",
    "property_types_count = {}\n",
    "entities = []\n",
    "for entity in entities_and_type.keys():\n",
    "    tipo = entities_and_type[entity][0]\n",
    "    if tipo != \"\":\n",
    "        entity_types_count[tipo] = entity_types_count.get(tipo, 0)+1\n",
    "        entities.append(entity)\n",
    "\n",
    "for subj in properties_and_types.keys():\n",
    "    for class_name, prop_name, prop_type, prop_value in properties_and_types[subj]:\n",
    "        property_types_count[(class_name, subj, prop_name, prop_type)] = property_types_count.get((class_name, subj, prop_name,prop_type), 0)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_dict = {t:{'count': 0} for t in entity_types_count.keys()}\n",
    "\n",
    "for class_name, subject,rel, p_type in property_types_count.keys():\n",
    "    index_dict[p_type] = {'count':0}\n",
    "    if class_name not in index_dict.keys():\n",
    "        index_dict[class_name] = {'count':0}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_triples.sort()\n",
    "\n",
    "#these two dictionaries contain the same indexes and \n",
    "#they have been split to populate faster edge_index\n",
    "subject_dict = {}\n",
    "object_dict = {}\n",
    "for triple in sorted(new_triples):\n",
    "    s = str(triple[0])\n",
    "    p = str(triple[1])\n",
    "    o = str(triple[2])\n",
    "    type_triples = []\n",
    "    if p != str(RDF.type):\n",
    "        s_type = entities_and_type[s][0] \n",
    "        p_type = get_type(p)\n",
    "        \n",
    "        if o.find(\"^^\") == -1:\n",
    "            o_type = entities_and_type[o][0]\n",
    "        else: \n",
    "            o_type = get_property_type(o)[0]\n",
    "        type_triples.append((s_type,p_type, o_type))\n",
    "\n",
    "        for s_type,p_type,o_type in type_triples:\n",
    "            if(s_type != \"\" and o_type != \"\"):\n",
    "                key_t = (s_type, p_type, o_type)\n",
    "\n",
    "                if key_t not in subject_dict.keys():\n",
    "                    subject_dict[key_t] = []\n",
    "                    object_dict[key_t] = []\n",
    "\n",
    "                if str(s) not in index_dict[s_type]:\n",
    "                    index_dict[s_type][str(s)] = index_dict[s_type]['count']\n",
    "                    index_dict[s_type]['count'] = index_dict[s_type]['count']+1\n",
    "                s_index = index_dict[s_type][str(s)]\n",
    "                \n",
    "       \n",
    "                if str(o) not in index_dict[o_type]:\n",
    "                    index_dict[o_type][str(o)] = index_dict[o_type]['count']\n",
    "                    index_dict[o_type]['count'] = index_dict[o_type]['count']+1\n",
    "                o_index = index_dict[o_type][str(o)]\n",
    "                \n",
    "                subject_dict[key_t].append(s_index)\n",
    "                object_dict[key_t].append(o_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "from dateutil.parser import parse\n",
    "import datetime, string\n",
    "\n",
    "def function_build_feature(p_type, value):\n",
    "    count = lambda l1, l2: len(list(filter(lambda c: c in l2, l1)))\n",
    "\n",
    "    if p_type == 'Integer':\n",
    "        try: i = int(value) \n",
    "        except: i = 0\n",
    "        return [i]\n",
    "    if p_type == 'Double':\n",
    "        try: d = float(value)\n",
    "        except: d = float(0.0)\n",
    "        return [d]\n",
    "    if p_type == 'gYear':\n",
    "        return [int(1970-value)]\n",
    "    if p_type == 'String':\n",
    "        a_punct = count(value, string.punctuation)\n",
    "        lang = 0\n",
    "        try:\n",
    "            if detect(value) == 'en': lang = 1\n",
    "        except:\n",
    "            lang = 0\n",
    "        return [len(value), value.count(\" \") , value.count(\"(\") + value.count(\")\"), lang, a_punct]\n",
    "    if p_type == 'Date':\n",
    "        return [(parse(value) - datetime.datetime(1970,1,1)).days]\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_data = HeteroData()\n",
    "\n",
    "data_to_insert = {}\n",
    "for subj in list(properties_and_types.keys()):\n",
    "    for class_type, prop_name, prop_type, prop_value in properties_and_types[subj]:\n",
    "        if prop_type not in data_to_insert:\n",
    "            data_to_insert[prop_type] = []\n",
    "        \n",
    "        p_count = property_types_count[(class_type, subj, prop_name, prop_type)]\n",
    "        for i in range(p_count):\n",
    "            data_to_insert[prop_type].append(function_build_feature(prop_type, prop_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = list(entity_types_count.keys())\n",
    "for t in types:\n",
    "    data_to_insert[t] = [[1] for i in range(entity_types_count[t])]\n",
    "\n",
    "for key in data_to_insert.keys():\n",
    "    lists = data_to_insert[key]\n",
    "    if lists != '':\n",
    "        complete_data[key].x = torch.Tensor(lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for triple in subject_dict.keys():\n",
    "    lol = [subject_dict[triple], object_dict[triple]]\n",
    "    if len(lol[0]) > 10:\n",
    "        complete_data[triple[0], triple[1], triple[2]].edge_index = torch.Tensor(lol).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import remove_self_loops, remove_isolated_nodes\n",
    "\n",
    "for edge_type in complete_data.edge_index_dict.keys():\n",
    "    if edge_type[0] == edge_type[2]:\n",
    "        new_edge_index = remove_self_loops(complete_data[edge_type].edge_index)[0]\n",
    "        complete_data[edge_type].edge_index = new_edge_index\n",
    "    new_edge_index = remove_isolated_nodes(complete_data[edge_type].edge_index)[0]\n",
    "    complete_data[edge_type].edge_index = new_edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_types = list(complete_data.edge_index_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_nodes_types = {}\n",
    "for node_type in complete_data.x_dict.keys():\n",
    "    i = 0\n",
    "    for edge_t in edge_types:\n",
    "        if node_type == edge_t[2]: break \n",
    "        i+=1\n",
    "    if i == len(edge_types):\n",
    "        root_nodes_types[node_type] = complete_data.x_dict[node_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAPPING FROM RELATION TYPE TO AN INT\n",
    "\n",
    "rel_to_index = {edge_t:i for edge_t,i in zip(edge_types,range(len(edge_types)))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_types = list(complete_data.x_dict.keys()) \n",
    "node_sizes = {} #dictionary with mapping node type: in_channels size. Useful to not use lazy inizialization, \n",
    "                #to allow reproducibility\n",
    "for k in node_types:\n",
    "    node_sizes[k] = len(complete_data[k].x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNN MODEL, TRAINING AND EVALUTION PROCEDURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import HeteroConv, GATConv, Linear\n",
    "\n",
    "class HeteroGNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels, edge_types, node_sizes, root_nodes_types):\n",
    "        super().__init__()\n",
    "        g = torch.manual_seed(0)\n",
    "        self.node_sizes = node_sizes\n",
    "        self.edge_types = edge_types\n",
    "        self.root_nodes_types = root_nodes_types\n",
    "        self.conv1 = HeteroConv({edge_t: GATConv((node_sizes[edge_t[0]], node_sizes[edge_t[2]]),\n",
    "                                                 hidden_channels,add_self_loops=False) for edge_t in edge_types})\n",
    "        self.conv2 = HeteroConv({edge_t: GATConv((hidden_channels if edge_t[0] not in root_nodes_types else node_sizes[edge_t[0]],\n",
    "                                                  hidden_channels),\n",
    "                                                 out_channels,add_self_loops=False) for edge_t in edge_types})\n",
    "        self.rel_weight = torch.nn.Parameter(torch.randn(len(edge_types), out_channels, generator=g))\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        self.conv1.reset_parameters()\n",
    "        self.conv2.reset_parameters()\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict, data):\n",
    "        x_dict = self.conv1(x_dict, edge_index_dict)\n",
    "        for t,v in root_nodes_types.items():\n",
    "            x_dict[t] = v\n",
    "        x_dict = {key: x.relu() for key, x in x_dict.items()}\n",
    "        \n",
    "        x_dict = self.conv2(x_dict, edge_index_dict)\n",
    "        for t,v in root_nodes_types.items():\n",
    "            x_dict[t] = v\n",
    "            \n",
    "        out = x_dict\n",
    "    \n",
    "        pred_dict = {}\n",
    "        ### LINK PREDICTION ACTS HERE ###\n",
    "        for edge_t in edge_types:\n",
    "            #Compute link embedding for each edge type\n",
    "            #for src in train_link[edge_t].edge_label_index[0]:\n",
    "            out_src = out[edge_t[0]][data[edge_t].edge_label_index[0]]#embedding src nodes\n",
    "            out_dst = out[edge_t[2]][data[edge_t].edge_label_index[1]] #embedding dst nodes\n",
    "        \n",
    "            # 2- Distmult with random initialized rel weights\n",
    "            #r = torch.Tensor([self.rel_weight[rel_to_index[edge_t]].detach().numpy() for z in range(len(out_src))])\n",
    "            out_sim = torch.sum(out_src * self.rel_weight[rel_to_index[edge_t]] * out_dst, dim=-1)\n",
    "            pred = out_sim\n",
    "        \n",
    "            pred_dict[edge_t] = pred\n",
    "        return pred_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "device = torch.device('cuda')\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HeteroGNN(hidden_channels=4, out_channels=2, \n",
    "                  edge_types=edge_types,\n",
    "                  node_sizes=node_sizes,\n",
    "                  root_nodes_types=root_nodes_types)\n",
    "\n",
    "model.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "\n",
    "link_split = RandomLinkSplit(num_val=0.0,\n",
    "                             num_test=0.25,\n",
    "                             edge_types=edge_types,\n",
    "                             rev_edge_types=[None]*len(edge_types))\n",
    "train_link, val_link, test_link = link_split(complete_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():  # Initialize lazy modules.\n",
    "    out = model(train_link.x_dict,train_link.edge_index_dict, train_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decay=5e-4\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=weight_decay)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01, weight_decay = weight_decay)\n",
    "#optimizer = torch.optim.RMSprop(model.parameters(), lr=0.1, weight_decay = weight_decay)\n",
    "#optimizer = torch.optim.Adagrad(model.parameters(), lr=0.01, weight_decay=weight_decay)\n",
    "criterion =  torch.nn.BCEWithLogitsLoss() #change loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_hetlinkpre():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()  # Clear gradients.\n",
    "    pred_dict = model(train_link.x_dict, train_link.edge_index_dict, train_link)  # Perform a single forward pass.\n",
    "    edge_labels = torch.Tensor()\n",
    "    preds = torch.Tensor()\n",
    "    for edge_t in edge_types:\n",
    "        preds = torch.cat((preds,pred_dict[edge_t]),-1)\n",
    "        edge_labels = torch.cat((edge_labels,train_link[edge_t].edge_label.type_as(pred_dict[edge_t])),-1)\n",
    "    #compute loss function based on all edge types\n",
    "    loss = criterion(preds, edge_labels)\n",
    "    loss.backward()  # Derive gradients.\n",
    "    optimizer.step()  # Update parameters based on gradients.\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "def test_hetlinkpre(test_link,evaluate='linkpre'):\n",
    "    if evaluate not in ['linkpre','propdetection','all']:\n",
    "        #linkpre: link between entities\n",
    "        #propdetection: link between an entity and a property\n",
    "        #all: entrambi \n",
    "        raise NotImplementedError()\n",
    "    model.eval()\n",
    "    hs_dict = model(test_link.x_dict, test_link.edge_index_dict, test_link)\n",
    "    hs = torch.Tensor([])\n",
    "    edge_labels = np.array([])\n",
    "    ### LINK PREDICTION ACTS HERE ###\n",
    "    #evaluate distincly entity-to-entity link prediction and entity-to-property(property-detection)\n",
    "    prop = ['String','Integer','Double','gYear','Date'] #add other property types if used\n",
    "    rel_with_prop = [edge_t for edge_t in edge_types if edge_t[2] in prop]\n",
    "    if evaluate == 'linkpre':\n",
    "        edge_types_to_evaluate = [edge_t for edge_t in edge_types if edge_t not in rel_with_prop]\n",
    "    elif evaluate == 'propdetection':\n",
    "        edge_types_to_evaluate = rel_with_prop\n",
    "    else:\n",
    "        edge_types_to_evaluate = edge_types\n",
    "    for edge_t in edge_types_to_evaluate:\n",
    "        hs = torch.cat((hs,hs_dict[edge_t]),-1)\n",
    "        edge_labels = np.concatenate((edge_labels,test_link[edge_t].edge_label.cpu().detach().numpy()))\n",
    "    \n",
    "    \n",
    "    pred_cont = torch.sigmoid(hs).cpu().detach().numpy()\n",
    "    \n",
    "    # EVALUATION\n",
    "    if evaluate=='propdetection':\n",
    "        test_roc_score = average_precision_score(edge_labels, pred_cont)\n",
    "    else:\n",
    "        test_roc_score = roc_auc_score(edge_labels, pred_cont) #comput AUROC score for test set\n",
    "    \n",
    "    return test_roc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model for a certain number of epochs, minimizing the loss\n",
    "perf_train = []\n",
    "perf_test = []\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    loss = train_hetlinkpre()\n",
    "    roc_train = test_hetlinkpre(train_link, evaluate='all')\n",
    "    roc_test = test_hetlinkpre(test_link, evaluate='all')\n",
    "    perf_train.append(roc_train)\n",
    "    perf_test.append(roc_test)\n",
    "    #print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, ROC train: {roc_train:.4f}, ROC test: {roc_test:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_train = test_hetlinkpre(train_link,evaluate='all')\n",
    "roc_test = test_hetlinkpre(test_link,evaluate='all')\n",
    "print(f'Train AUROC: {roc_train:.4f}\\nTest AUROC: {roc_test:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = range(num_epochs)\n",
    "plt.clf()\n",
    "plt.plot(x, perf_train, color='orange', label='auroc_train')\n",
    "plt.plot(x, perf_test, color='blue', label='auroc_test')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('AUROC-score')\n",
    "plt.legend()\n",
    "plt.ylim(top=1)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "#plt.clf()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "ed8a8f81fdcbe4bb580e703d3d81905dffa90d7a53be09b673d3f1a6169b3bc5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
